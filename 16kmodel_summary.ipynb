{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp4crlyIa2fR+rYsp0k82c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyanta012/demo/blob/main/16kmodel_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VPK6iaeLnA4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain==0.0.205\n",
        "!pip install tiktoken==0.3.3\n",
        "!pip install openai==0.27.2\n",
        "!pip install pypdf==3.10.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import tiktoken\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# APIKEYå–å¾—\n",
        "APIKEY = getpass.getpass(\"APIKEY: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = APIKEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ_Pxp8zL_Kw",
        "outputId": "cdd01390-cf41-46ba-acc6-61622caee249"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "APIKEY: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrDfdYm7MN4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        ")"
      ],
      "metadata": {
        "id": "Uw00ACQ0Ma_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template = \"\"\"\n",
        "    ä¸‹è¨˜ã®#ã§åŒºåˆ‡ã‚‰ã‚ŒãŸæ–‡ç« ã‚’è‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„\n",
        "\n",
        "    ####\n",
        "    {text}\n",
        "    ####\n",
        "    \"\"\"\n",
        "\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(text=\"ã“ã‚“ã«ã¡ã¯\").to_messages()"
      ],
      "metadata": {
        "id": "MHhxyndrMbD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WapWuCpcNJlG",
        "outputId": "10f6c3bc-1681-4d07-9bad-3f45c32a9e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lvOZk6d2MdHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenã®æ•°ã‚’æ•°ãˆã‚‹ã“ã¨ãŒã§ãã‚‹\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "BH8CxHMUMdLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"ã­ã“ã‹ã„ãŸã„\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8C94p2vPz4B",
        "outputId": "e4e748d5-f78b-467c-de2e-2d47148d064d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2243, 255, 22957, 32149, 16995, 28713, 16995]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HS8g_rLcMiNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template = \"\"\"\n",
        "    ä¸‹è¨˜ã®#ã§åŒºåˆ‡ã‚‰ã‚ŒãŸå˜èªã‚’åå¯¾ã‹ã‚‰å‡ºåŠ›ã—ã¦ãã ã•ã„\n",
        "\n",
        "    ####\n",
        "    {text}\n",
        "    ####\n",
        "    \"\"\"\n",
        "\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(text=\"ã­ã“ã‹ã„ãŸã„\").to_messages()"
      ],
      "metadata": {
        "id": "P5jFuBDeP8AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7NyknsfPqDv",
        "outputId": "90d2e3f7-4e98-4824-959e-9d9688e04be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ã„ãŸã„ã‹ã“ã­"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23wJJ7ZWMjgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ivd0EzuvMjkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IwKOYFZmMjny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# è¦ç´„å‡¦ç†ã®å®Ÿè£…"
      ],
      "metadata": {
        "id": "6dA1XhVIMjqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_token(text):\n",
        "    print(f\"Total token: {len(encoding.encode(text, disallowed_special=()))}\")\n",
        "\n",
        "\n",
        "def preprocess_text(loader):\n",
        "    docs = loader.load_and_split()\n",
        "    cnt_token = 0\n",
        "    content = \"\"\n",
        "    for i in range(len(docs)):\n",
        "        page_content = docs[i].page_content\n",
        "        cnt_token += len(encoding.encode(page_content, disallowed_special=()))\n",
        "        if cnt_token > 13_000:\n",
        "            break\n",
        "        content += page_content\n",
        "    count_token(content)\n",
        "    return content"
      ],
      "metadata": {
        "id": "dSdvJQtbMnKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarizer(content, use_system_message=True):\n",
        "    template = \"ã‚ãªãŸã¯AIæ©Ÿæ¢°å­¦ç¿’ãŒå°‚é–€ã®ç ”ç©¶è€…ã§ã™ã€‚å‡ºåŠ›ã¯å…¨ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ã€å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ãã‚Œã«ç´ã¥ãå†…å®¹ã‚’æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¾ã™ã€‚\"\n",
        "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "    human_template = \"\"\"\n",
        "    è«–æ–‡: {text}\n",
        "\n",
        "    ä¸Šè¨˜ã®è«–æ–‡ã«ãŠã„ã¦ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚¿ã‚¹ã‚¯ã‚’è¡Œã£ã¦ãã ã•ã„\n",
        "    1. å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹\n",
        "    2. ãã®ã‚¿ã‚¤ãƒˆãƒ«ã®è¦ç´„ã‚’æ˜ç¢ºã«æ„å‘³ãŒã‚ã‹ã‚‹ã‚ˆã†ã«è¡Œã†ã€‚å¿…è¦ã«å¿œã˜ã¦å°‚é–€çŸ¥è­˜ã§è£œå®Œã™ã‚‹ã€‚\n",
        "\n",
        "    \"\"\"\n",
        "    if not use_system_message:\n",
        "        human_template = template + human_template\n",
        "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "    if not use_system_message:\n",
        "        chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "    else:\n",
        "        chat_prompt = ChatPromptTemplate.from_messages(\n",
        "            [system_message_prompt, human_message_prompt]\n",
        "        )\n",
        "    prompt = chat_prompt.format_prompt(text=content).to_messages()\n",
        "    llm = ChatOpenAI(\n",
        "        temperature=0,\n",
        "        model=\"gpt-3.5-turbo-16k\",\n",
        "        streaming=True,\n",
        "        callbacks=[StreamingStdOutCallbackHandler()],\n",
        "    )\n",
        "    return llm(prompt)"
      ],
      "metadata": {
        "id": "tA04FDp_MnRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5Gwa5Pw3vxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"lora.pdf\")"
      ],
      "metadata": {
        "id": "BVaDVUS7Mpij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = preprocess_text(loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTqCJsovMpmx",
        "outputId": "bb9b3b12-ea15-42bd-8d69-5169e4c0b6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total token: 12131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "pHK5M1WiTcyY",
        "outputId": "2b73f439-6088-4458-d51f-1ae19552b20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LORA: L OW-RANK ADAPTATION OF LARGE LAN-\\nGUAGE MODELS\\nEdward Hu\\x03Yelong Shen\\x03Phillip Wallis Zeyuan Allen-Zhu\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\nMicrosoft Corporation\\nfedwardhu, yeshe, phwallis, zeyuana,\\nyuanzhil, swang, luw, wzchen g@microsoft.com\\nyuanzhil@andrew.cmu.edu\\n(Version 2)\\nABSTRACT\\nAn important paradigm of natural language processing consists of large-scale pre-\\ntraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger models, full ï¬ne-tuning, which retrains all model parameters,\\nbecomes less feasible. Using GPT-3 175B as an example â€“ deploying indepen-\\ndent instances of ï¬ne-tuned models, each with 175B parameters, is prohibitively\\nexpensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\\ntrained model weights and injects trainable rank decomposition matrices into each\\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-\\nrameters for downstream tasks. Compared to GPT-3 175B ï¬ne-tuned with Adam,\\nLoRA can reduce the number of trainable parameters by 10,000 times and the\\nGPU memory requirement by 3 times. LoRA performs on-par or better than ï¬ne-\\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\\nno additional inference latency . We also provide an empirical investigation into\\nrank-deï¬ciency in language model adaptation, which sheds light on the efï¬cacy of\\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\\nmodels and provide our implementations and model checkpoints for RoBERTa,\\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\\n1 I NTRODUCTION\\nPretrained \\nWeights\\nğ‘Šâˆˆâ„ğ‘‘Ã—ğ‘‘\\nxh\\nğµ=0\\nğ´=ğ’©(0,ğœ2)\\nğ‘‘ğ‘ŸPretrained \\nWeights\\nğ‘Šâˆˆâ„ğ‘‘Ã—ğ‘‘\\nxf(x)\\nğ‘‘\\nFigure 1: Our reparametriza-\\ntion. We only train AandB.Many applications in natural language processing rely on adapt-\\ningonelarge-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ï¬ne-tuning ,\\nwhich updates all the parameters of the pre-trained model. The ma-\\njor downside of ï¬ne-tuning is that the new model contains as many\\nparameters as in the original model. As larger models are trained\\nevery few months, this changes from a mere â€œinconvenienceâ€ for\\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\\n175 billion trainable parameters.1\\nMany sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciï¬c parameters in ad-\\ndition to the pre-trained model for each task, greatly boosting the\\noperational efï¬ciency when deployed. However, existing techniques\\n\\x03Equal contribution.\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, ï¬ne-tuning boosts its perfor-\\nmance signiï¬cantly as shown in Appendix A.\\n1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021often introduce inference latency (Houlsby et al., 2019; Rebufï¬ et al., 2017) by extending model\\ndepth or reduce the modelâ€™s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\\nbardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\\nmatch the ï¬ne-tuning baselines, posing a trade-off between efï¬ciency and model quality.\\nWe take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low â€œintrinsic rankâ€, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layersâ€™ change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\\n175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufï¬ces even\\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efï¬cient.\\nLoRA possesses several key advantages.\\nâ€¢ A pre-trained model can be shared and used to build many small LoRA modules for dif-\\nferent tasks. We can freeze the shared model and efï¬ciently switch tasks by replacing the\\nmatricesAandBin Figure 1, reducing the storage requirement and task-switching over-\\nhead signiï¬cantly.\\nâ€¢ LoRA makes training more efï¬cient and lowers the hardware barrier to entry by up to 3\\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\\nmuch smaller low-rank matrices.\\nâ€¢ Our simple linear design allows us to merge the trainable matrices with the frozen weights\\nwhen deployed, introducing no inference latency compared to a fully ï¬ne-tuned model, by\\nconstruction.\\nâ€¢ LoRA is orthogonal to many prior methods and can be combined with many of them, such\\nas preï¬x-tuning. We provide an example in Appendix E.\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\\nquery/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\\ntrained weight matrix and \\x01Wits accumulated gradient update during adaptation. We use rto\\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\\noptimization and use a Transformer MLP feedforward dimension dffn= 4\\x02dmodel .\\n2 P ROBLEM STATEMENT\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivat-\\ning use case. Below is a brief description of the language modeling problem and, in particular, the\\nmaximization of conditional probabilities given a task-speciï¬c prompt.\\nSuppose we are given a pre-trained autoregressive language model P\\x08(yjx)parametrized by \\x08.\\nFor instance, P\\x08(yjx)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\\net al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\\npre-trained model to downstream conditional text generation tasks, such as summarization, machine\\nreading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\\nrepresented by a training dataset of context-target pairs: Z=f(xi;yi)gi=1;::;N, where both xiand\\nyiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\\ncorresponding SQL command; for summarization, xiis the content of an article and yiits summary.\\n2During full ï¬ne-tuning, the model is initialized to pre-trained weights \\x080and updated to \\x080+ \\x01\\x08\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\n\\x08X\\n(x;y)2ZjyjX\\nt=1log(P\\x08(ytjx;y<t)) (1)\\nOne of the main drawbacks for full ï¬ne-tuning is that for each downstream task, we learn a different\\nset of parameters \\x01\\x08whose dimensionj\\x01\\x08jequalsj\\x080j. Thus, if the pre-trained model is large\\n(such as GPT-3 with j\\x080j\\x19175Billion), storing and deploying many independent instances of\\nï¬ne-tuned models can be challenging, if at all feasible.\\nIn this paper, we adopt a more parameter-efï¬cient approach, where the task-speciï¬c parameter\\nincrement \\x01\\x08 = \\x01\\x08(\\x02) is further encoded by a much smaller-sized set of parameters \\x02with\\nj\\x02j\\x1cj \\x080j. The task of ï¬nding \\x01\\x08thus becomes optimizing over \\x02:\\nmax\\n\\x02X\\n(x;y)2ZjyjX\\nt=1log\\x00\\np\\x080+\\x01\\x08(\\x02) (ytjx;y<t)\\x01\\n(2)\\nIn the subsequent sections, we propose to use a low-rank representation to encode \\x01\\x08that is both\\ncompute- and memory-efï¬cient. When the pre-trained model is GPT-3 175B, the number of train-\\nable parametersj\\x02jcan be as small as 0:01% ofj\\x080j.\\n3 A RENâ€™TEXISTING SOLUTIONS GOOD ENOUGH ?\\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\\nof works have sought to make model adaptation more parameter- and compute-efï¬cient. See Sec-\\ntion 6 for a survey of some of the well-known works. Using language modeling as an example, there\\nare two prominent strategies when it comes to efï¬cient adaptations: adding adapter layers (Houlsby\\net al., 2019; Rebufï¬ et al., 2017; Pfeiffer et al., 2021; R Â¨ucklÂ´e et al., 2020) or optimizing some forms\\nof the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\\nLiu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\\nlatency-sensitive production scenario.\\nAdapter Layers Introduce Inference Latency There are many variants of adapters. We focus\\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\\ning multi-task settings (R Â¨ucklÂ´e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\\na difference in the online inference setting where the batch size is typically as small as one. In a\\ngeneric scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\\nvery small bottleneck dimension (Table 1).\\nThis problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\\nAllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\\nDirectly Optimizing the Prompt is Hard The other direction, as exempliï¬ed by preï¬x tuning (Li\\n& Liang, 2021), faces a different challenge. We observe that preï¬x tuning is difï¬cult to optimize\\nand that its performance changes non-monotonically in trainable parameters, conï¬rming similar\\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for\\nadaptation necessarily reduces the sequence length available to process a downstream task, which\\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the studyadaptation necessarily reduces the sequence length available to process a downstream task, which\\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the study\\non task performance to Section 5.\\n3Batch Size 32 16 1\\nSequence Length 512 256 128\\nj\\x02j 0.5M 11M 11M\\nFine-Tune/LoRA 1449.4\\x060.8 338.0 \\x060.6 19.8 \\x062.7\\nAdapterL1482.0\\x061.0 (+2.2%) 354.8 \\x060.5 (+5.0%) 23.9 \\x062.1 (+20.7%)\\nAdapterH1492.2\\x061.0 (+3.0%) 366.3 \\x060.5 (+8.4%) 25.8 \\x062.2 (+30.3%)\\nTable 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. â€œ j\\x02jâ€ denotes the number of trainable\\nparameters in adapter layers. AdapterLand AdapterHare two variants of adapter tuning, which we\\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiï¬cant in an\\nonline, short-sequence-length scenario. See the full study in Appendix B.\\n4 O URMETHOD\\nWe describe the simple design of LoRA and its practical beneï¬ts. The principles outlined here apply\\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\\nlanguage models in our experiments as the motivating use case.\\n4.1 L OW-RANK -PARAMETRIZED UPDATE MATRICES\\nA neural network contains many dense layers which perform matrix multiplication. The weight\\nmatrices in these layers typically have full-rank. When adapting to a speciï¬c task, Aghajanyan et al.\\n(2020) shows that the pre-trained language models have a low â€œinstrisic dimensionâ€ and can still\\nlearn efï¬ciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\\nsize the updates to the weights also have a low â€œintrinsic rankâ€ during adaptation. For a pre-trained\\nweight matrix W02Rd\\x02k, we constrain its update by representing the latter with a low-rank de-\\ncomposition W0+ \\x01W=W0+BA, whereB2Rd\\x02r;A2Rr\\x02k, and the rank r\\x1cmin(d;k).\\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\\nparameters. Note both W0and\\x01W=BAare multiplied with the same input, and their respective\\noutput vectors are summed coordinate-wise. For h=W0x, our modiï¬ed forward pass yields:\\nh=W0x+ \\x01Wx=W0x+BAx (3)\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\\nzero forB, so\\x01W=BAis zero at the beginning of training. We then scale \\x01Wx by\\x0b\\nr, where\\x0b\\nis a constant in r. When optimizing with Adam, tuning \\x0bis roughly the same as tuning the learning\\nrate if we scale the initialization appropriately. As a result, we simply set \\x0bto the ï¬rstrwe try\\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\\nr(Yang & Hu, 2021).\\nA Generalization of Full Fine-tuning. A more general form of ï¬ne-tuning allows the training of\\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\\nness of full ï¬ne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\\nto training the original model, while adapter-based methods converges to an MLP and preï¬x-based\\nmethods to a model that cannot take long input sequences.\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd\\x02k.\\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\\nthen adding a different B0A0, a quick operation with very little memory overhead. Critically, this\\n2They represent a negligible number of parameters compared to weights.\\n3An inevitability when adapting to hard tasks.\\n4guarantees that we do not introduce any additional latency during inference compared to a ï¬ne-tuned\\nmodel by construction.\\n4.2 A PPLYING LORA TOTRANSFORMER\\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\\nthe self-attention module ( Wq;Wk;Wv;Wo) and two in the MLP module. We treat Wq(orWk,Wv)\\nas a single matrix of dimension dmodel\\x02dmodel , even though the output dimension is usually sliced\\ninto attention heads. We limit our study to only adapting the attention weights for downstream\\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\\nand parameter-efï¬ciency.We further study the effect on adapting different types of attention weight\\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\\nlayers, LayerNorm layers, and biases to a future work.\\nPractical Beneï¬ts and Limitations. The most signiï¬cant beneï¬t comes from the reduction in\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\nusage by up to 2=3ifr\\x1cdmodel as we do not need to store the optimizer states for the frozen\\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\\n350GB. With r= 4and only the query and value projection matrices being adapted, the checkpoint\\nsize is reduced by roughly 10,000 \\x02(from 350GB to 35MB)4. This allows us to train with signiï¬-\\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneï¬t is that we can switch between tasks\\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\\nparameters. This allows for the creation of many customized models that can be swapped in and out\\non the ï¬‚y on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\\nduring training on GPT-3 175B compared to full ï¬ne-tuning5as we do not need to calculate the\\ngradient for the vast majority of the parameters.\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\\nwith different AandBin a single forward pass, if one chooses to absorb AandBintoWto eliminate\\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n5 E MPIRICAL EXPERIMENTS\\nWe evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\\n(NLU) to generation (NLG). Speciï¬cally, we evaluate on the GLUE (Wang et al., 2019) benchmark\\nfor RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n5.1 B ASELINES\\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their\\nreported numbers whenever possible. This, however, means that some baselines might only appear\\nin certain experiments.\\nFine-Tuning (FT) is a common approach for adaptation. During ï¬ne-tuning, the model is initialized\\nto the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\\nvariant is to update only some layers while freezing others. We include one such baseline reported\\nin prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers ( FTTop2).\\n4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\\n350GB + 35MB * 100 \\x19354GB as opposed to 100 * 350GB \\x1935TB.4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\\n350GB + 35MB * 100 \\x19354GB as opposed to 100 * 350GB \\x1935TB.\\n5For GPT-3 175B, the training throughput for full ï¬ne-tuning is 32.5 tokens/s per V100 GPU; with the same\\nnumber of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.\\n5Model & Method # Trainable\\nParameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\\nRoB base(FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4\\nRoB base(BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2\\nRoB base(AdptD)* 0.3M 87.1\\x06.094.2\\x06.188.5\\x061.160.8\\x06.493.1\\x06.190.2\\x06.071.5\\x062.789.7\\x06.384.4\\nRoB base(AdptD)* 0.9M 87.3\\x06.194.7\\x06.388.4\\x06.162.6\\x06.993.0\\x06.290.6\\x06.075.9\\x062.290.3\\x06.185.4\\nRoB base(LoRA) 0.3M 87.5\\x06.395.1\\x06.289.7\\x06.763.4\\x061.293.3\\x06.390.8\\x06.186.6\\x06.791.5\\x06.287.2\\nRoB large(FT)* 355.0M 90.2 96.4 90.9 68.0 94.7 92.2 86.6 92.4 88.9\\nRoB large(LoRA) 0.8M 90.6\\x06.296.2\\x06.590.9\\x061.268.2\\x061.994.9\\x06.391.6\\x06.187.4\\x062.592.6\\x06.289.0\\nRoB large(AdptP)y 3.0M 90.2\\x06.396.1\\x06.390.2\\x06.768.3\\x061.094.8\\x06.291.9\\x06.183.8\\x062.992.1\\x06.788.4\\nRoB large(AdptP)y 0.8M 90.5\\x06.396.6\\x06.289.7\\x061.267.8\\x062.594.8\\x06.391.7\\x06.280.1\\x062.991.9\\x06.487.9\\nRoB large(AdptH)y 6.0M 89.9\\x06.596.2\\x06.388.7\\x062.966.5\\x064.494.7\\x06.292.1\\x06.183.4\\x061.191.0\\x061.787.8\\nRoB large(AdptH)y 0.8M 90.3\\x06.396.3\\x06.587.7\\x061.766.3\\x062.094.7\\x06.291.5\\x06.172.9\\x062.991.5\\x06.586.4\\nRoB large(LoRA)y 0.8M 90.6\\x06.296.2\\x06.590.2\\x061.068.2\\x061.994.8\\x06.391.6\\x06.285.2\\x061.192.3\\x06.588.6\\nDeB XXL(FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1\\nDeB XXL(LoRA) 4.7M 91.9\\x06.296.9\\x06.292.6\\x06.672.4\\x061.196.0\\x06.192.9\\x06.194.9\\x06.493.0\\x06.291.3\\nTable 2: RoBERTa base, RoBERTa large, and DeBERTa XXLwith different adaptation methods on the\\nGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthewâ€™s\\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better\\nfor all metrics. * indicates numbers published in prior works. yindicates runs conï¬gured in a setup\\nsimilar to Houlsby et al. (2019) for a fair comparison.\\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\\nContemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\\nPreï¬x-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe-\\ncial tokens have trainable word embeddings and are generally not in the modelâ€™s vocabulary. Where\\nto place such tokens can have an impact on performance. We focus on â€œpreï¬xingâ€, which prepends\\nsuch tokens to the prompt, and â€œinï¬xingâ€, which appends to the prompt; both are discussed in Li &\\nLiang (2021). We use lp(resp.li) denote the number of preï¬x (resp. inï¬x) tokens. The number of\\ntrainable parameters is j\\x02j=dmodel\\x02(lp+li).\\nPreï¬x-layer tuning (PreLayer) is an extension to preï¬x-embedding tuning. Instead of just learning\\nthe word embeddings (or equivalently, the activations after the embedding layer) for some special\\ntokens, we learn the activations after every Transformer layer. The activations computed from pre-\\nvious layers are simply replaced by trainable ones. The resulting number of trainable parameters is\\nj\\x02j=L\\x02dmodel\\x02(lp+li), whereLis the number of Transformer layers.\\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-\\nattention module (and the MLP module) and the subsequent residual connection. There are two\\nfully connected layers with biases in an adapter layer with a nonlinearity in between. We call this\\noriginal design AdapterH. Recently, Lin et al. (2020) proposed a more efï¬cient design with the\\nadapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This\\nis very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also\\ninclude another baseline call AdapterDrop (R Â¨ucklÂ´e et al., 2020) which drops some adapter layers for\\ngreater efï¬ciency ( AdapterD). We cite numbers from prior works whenever possible to maximize\\nthe number of baselines we compare with; they are in rows with an asterisk (*) in the ï¬rst column.\\nIn all cases, we have j\\x02j=^LAdpt\\x02(2\\x02dmodel\\x02r+r+dmodel )+2\\x02^LLN\\x02dmodel where ^LAdpt\\nis the number of adapter layers and ^LLNthe number of trainable LayerNorms (e.g., in AdapterL).\\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.is the number of adapter layers and ^LLNthe number of trainable LayerNorms (e.g., in AdapterL).\\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\\nAs mentioned in Section 4.2, we only apply LoRA to WqandWvin most experiments for simplicity.\\nThe number of trainable parameters is determined by the rank rand the shape of the original weights:\\nj\\x02j= 2\\x02^LLoRA\\x02dmodel\\x02r, where ^LLoRA is the number of weight matrices we apply LoRA to.\\n6Model & Method # Trainable E2E NLG Challenge\\nParameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47\\nGPT-2 M (AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40\\nGPT-2 M (AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47\\nGPT-2 M (AdapterH) 11.09M 67.3\\x06.68.50\\x06.07 46.0\\x06.2 70.7\\x06.2 2.44\\x06.01\\nGPT-2 M (FTTop2)* 25.19M 68.1 8.59 46.0 70.8 2.41\\nGPT-2 M (PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49\\nGPT-2 M (LoRA) 0.35M 70.4\\x06.18.85\\x06.02 46.8\\x06.2 71.8\\x06.1 2.53\\x06.02\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (AdapterL) 0.88M 69.1\\x06.18.68\\x06.03 46.3\\x06.0 71.4\\x06.2 2.49\\x06.0\\nGPT-2 L (AdapterL) 23.00M 68.9\\x06.38.70\\x06.04 46.1\\x06.1 71.3\\x06.2 2.45\\x06.02\\nGPT-2 L (PreLayer)* 0.77M 70.3 8.85 46.2 71.7 2.47\\nGPT-2 L (LoRA) 0.77M 70.4\\x06.18.89\\x06.02 46.8\\x06.2 72.0\\x06.2 2.47\\x06.02\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable\\nor fewer trainable parameters. Conï¬dence intervals are shown for experiments we ran. * indicates\\nnumbers published in prior works.\\n5.2 R OBERT A BASE /LARGE\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\\net al., 2019a) and boosted the latterâ€™s task performance without introducing many more trainable\\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\\nand evaluate the performance of different efï¬cient adaptation approaches on tasks from the GLUE\\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their\\nsetup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when\\ncomparing with adapters. First, we use the same batch size for all tasks and use a sequence length\\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the ï¬ne-tuning baseline. Runs\\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with y. The result is\\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\\n5.3 D EBERT AXXL\\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\\nï¬ne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).\\nSee Section D.2 for details on the hyperparameters used.\\n5.4 GPT-2 MEDIUM /LARGE\\nHaving shown that LoRA can be a competitive alternative to full ï¬ne-tuning on NLU, we hope to\\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\\nto space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section.\\nSee Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We\\ninclude a list of the hyperparameters used in Section D.3.\\n7Model&Method# Trainable WikiSQL MNLI-m SAMSum\\nParameters Acc. (%) Acc. (%) R1/R2/RL\\nGPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5\\nGPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5\\nGPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5\\nGPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5\\nGPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8\\nGPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1\\nGPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9\\nGPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\\nSAMSum. LoRA performs better than prior approaches, including full ï¬ne-tuning. The results\\non WikiSQL have a ï¬‚uctuation around \\x060:5%, MNLI-m around \\x060:1%, and SAMSum around\\n\\x060:2/\\x060:2/\\x060:1for the three metrics.\\n5.5 S CALING UP TO GPT-3 175B\\nAs a ï¬nal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high\\ntraining cost, we only report the typical standard deviation for a given task over random seeds, as\\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\nAs shown in Table 4, LoRA matches or exceeds the ï¬ne-tuning baseline on all three datasets. Note\\nthat not all methods beneï¬t monotonically from having more trainable parameters, as shown in Fig-\\nure 2. We observe a signiï¬cant performance drop when we use more than 256 special tokens for\\npreï¬x-embedding tuning or more than 32 special tokens for preï¬x-layer tuning. This corroborates\\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\\nperformance of different adaptation approaches in the low-data regime in Section F.3.\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.550.600.650.700.75Validation Accuracy\\nWikiSQL\\nMethod\\nFine-Tune\\nPrefixEmbed\\nPrefixLayer\\nAdapter(H)\\nLoRA\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.840.860.880.900.92\\nMultiNLI-matched\\nFigure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation\\nmethods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance.\\nSee Section F.2 for more details on the plotted data points.\\n6 R ELATED WORKS\\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence\\narchitecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\\nguage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\\nmodels have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged\\nwith BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) â€“ both are large Transformer lan-\\n8guage models trained on a large amount of text â€“ where ï¬ne-tuning on task-speciï¬c data after pre-\\ntraining on general domain data provides a signiï¬cant performance gain compared to training on\\ntask-speciï¬c data directly. Training larger Transformers generally results in better performance and\\nremains an active research direction. GPT-3 (Brown et al., 2020) is the largest single Transformer\\nlanguage model trained to-date with 175B parameters.\\nPrompt Engineering and Fine-Tuning. While GPT-3 175B can adapt its behavior with just a\\nfew additional training examples, the result depends heavily on the input prompt (Brown et al.,\\n2020). This necessitates an empirical art of composing and formatting the prompt to maximize a\\nmodelâ€™s performance on a desired task, which is known as prompt engineering or prompt hacking.\\nFine-tuning retrains a model pre-trained on general domains to a speciï¬c task Devlin et al. (2019b);\\nRadford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b);\\nCollobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream\\nperformance. However, the enormity of GPT-3 175B makes it challenging to perform ï¬ne-tuning in\\nthe usual way due to the large checkpoint it produces and the high hardware barrier to entry since it\\nhas the same memory footprint as pre-training.\\nParameter-Efï¬cient Adaptation. Many have proposed inserting adapter layers between existing\\nlayers in a neural network (Houlsby et al., 2019; Rebufï¬ et al., 2017; Lin et al., 2020). Our method\\nuses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The\\nkey functional difference is that our learned weights can be merged with the main weights during\\ninference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).\\nA comtenporary extension of adapter is COMPACTER (Mahabadi et al., 2021), which essentially\\nparametrizes the adapter layers using Kronecker products with some predetermined weight sharing\\nscheme. Similarly, combining LoRA with other tensor product-based methods could potentially\\nimprove its parameter efï¬ciency, which we leave to future work. More recently, many proposed\\noptimizing the input word embeddings in lieu of ï¬ne-tuning, akin to a continuous and differentiable\\ngeneralization of prompt engineering (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al.,\\n2020; Liu et al., 2021). We include comparisons with Li & Liang (2021) in our experiment section.\\nHowever, this line of works can only scale up by using more special tokens in the prompt, which\\ntake up available sequence length for task tokens when positional embeddings are learned.\\nLow-Rank Structures in Deep Learning. Low-rank structure is very common in machine learn-\\ning. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016;\\nCai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many\\ndeep learning tasks, especially those with a heavily over-parametrized neural network, the learned\\nneural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works\\neven explicitly impose the low-rank constraint when training the original neural network (Sainath\\net al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\\ndak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\\nconsiders low-rank update to a frozen model for adaptation to downstream tasks . In theory liter-\\nature, it is known that neural networks outperform other classical learning methods, including the\\ncorresponding (ï¬nite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when\\nthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests thatthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that\\nlow-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed\\nlow-rank adaptation update is well-motivated by the literature.\\n7 U NDERSTANDING THE LOW-RANK UPDATES\\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank\\nadaptation learned from downstream tasks. Note that the low-rank structure not only lowers the\\nhardware barrier to entry which allows us to run multiple experiments in parallel, but also gives\\nbetter interpretability of how the update weights are correlated with the pre-trained weights. We\\nfocus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters\\n(up to 10,000\\x02) without adversely affecting task performances.\\nWe perform a sequence of empirical studies to answer the following questions: 1) Given a parameter\\nbudget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt\\n9to maximize downstream performance? 2) Is the â€œoptimalâ€ adaptation matrix \\x01Wreally rank-\\ndeï¬cient ? If so, what is a good rank to use in practice? 3) What is the connection between \\x01Wand\\nW? Does \\x01Whighly correlate with W? How large is \\x01Wcomparing to W?\\nWe believe that our answers to question (2) and (3) shed light on the fundamental principles of using\\npre-trained language models for downstream tasks, which is a critical topic in NLP.\\n7.1 W HICH WEIGHT MATRICES IN TRANSFORMER SHOULD WEAPPLY LORA TO?\\nGiven a limited parameter budget, which types of weights should we adapt with LoRA to obtain\\nthe best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight\\nmatrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored\\nin FP16) on GPT-3 175B, which corresponds to r= 8if we adapt one type of attention weights or\\nr= 4if we adapt two types, for all 96 layers. The result is presented in Table 5.\\n# of Trainable Parameters = 18M\\nWeight Type WqWkWvWoWq;WkWq;WvWq;Wk;Wv;Wo\\nRankr 8 8 8 8 4 4 2\\nWikiSQL (\\x060:5%) 70.4 70.0 73.0 73.2 71.4 73.7 73.7\\nMultiNLI (\\x060:1%) 91.0 90.8 91.0 91.3 91.3 91.3 91.7\\nTable 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of\\nattention weights in GPT-3, given the same number of trainable parameters. Adapting both Wqand\\nWvgives the best performance overall. We ï¬nd the standard deviation across random seeds to be\\nconsistent for a given dataset, which we report in the ï¬rst column.\\nNote that putting all the parameters in \\x01Wqor\\x01Wkresults in signiï¬cantly lower performance,\\nwhile adapting both WqandWvyields the best result. This suggests that even a rank of four\\ncaptures enough information in \\x01Wsuch that it is preferable to adapt more weight matrices than\\nadapting a single type of weights with a larger rank.\\n7.2 W HAT IS THE OPTIMAL RANKrFOR LORA?\\nWe turn our attention to the effect of rank ron model performance. We adapt fWq;Wvg,\\nfWq;Wk;Wv;Wcg, and justWqfor a comparison.\\nWeight Type r= 1r= 2r= 4r= 8r= 64\\nWikiSQL(\\x060:5%)Wq 68.8 69.6 70.5 70.4 70.0\\nWq;Wv 73.4 73.3 73.7 73.8 73.5\\nWq;Wk;Wv;Wo 74.1 73.7 74.0 74.0 73.9\\nMultiNLI (\\x060:1%)Wq 90.7 90.9 91.1 90.7 90.7\\nWq;Wv 91.3 91.4 91.3 91.6 91.4\\nWq;Wk;Wv;Wo 91.2 91.7 91.7 91.5 91.4\\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a\\nrank as small as one sufï¬ces for adapting both WqandWvon these datasets while training Wqalone\\nneeds a larger r. We conduct a similar experiment on GPT-2 in Section H.2.\\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small r(more\\nso forfWq;Wvgthan justWq). This suggests the update matrix \\x01Wcould have a very small\\nâ€œintrinsic rankâ€.6To further support this ï¬nding, we check the overlap of the subspaces learned by\\ndifferent choices of rand by different random seeds. We argue that increasing rdoes not cover a\\nmore meaningful subspace, which suggests that a low-rank adaptation matrix is sufï¬cient.\\n6However, we do not expect a small rto work for every task or dataset. Consider the following thought\\nexperiment: if the downstream task were in a different language than the one used for pre-training, retraining\\nthe entire model (similar to LoRA with r=dmodel ) could certainly outperform LoRA with a small r.\\n10Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1\\x14i\\x148) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1\\x14j\\x1464)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\n\\x1e(Ar=8;Ar=64;i;j) =jjUi>\\nAr=8Uj\\nAr=64jj2\\nF\\nmin(i;j)2[0;1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\n\\x1e(\\x01)has a range of [0;1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how \\x1echanges as we vary iandj. We only look at the 48th layer\\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both \\x01Wqand\\x01Wv.\\nThe third and the fourth ï¬gures zoom in on the lower-left triangle in the ï¬rst two ï¬gures. The top\\ndirections in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiï¬cantly between\\nAr=8andAr=64, while others do not. Speciï¬cally, \\x01Wv(resp. \\x01Wq) ofAr=8\\nand\\x01Wv(resp. \\x01Wq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0:5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\\ncan indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conï¬rm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n\\x01Wqappears to have a higher â€œintrinsic rankâ€ than \\x01Wv, since more common singular value direc-\\ntions are learned by both runs for \\x01Wq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX \\x01WCOMPARE TO W?\\nWe further investigate the relationship between \\x01WandW. In particular, does \\x01Whighly correlate\\nwithW? (Or mathematically, is \\x01Wmostly contained in the top singular directions of W?) Also,\\n7Note that a similar analysis can be carried out with Band the left-singular unitary matrices â€“ we stick with\\nAfor our experiments.\\n11'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tW_87FXOMpqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ã‚ãªã„å ´åˆ"
      ],
      "metadata": {
        "id": "2pcOqQLLMptG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = summarizer(content, use_system_message=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZiRKMqnMnUw",
        "outputId": "91018107-41b3-46e3-993b-53bd5291322d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ABSTRACT\n",
            "- å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’ã¨ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®é©å¿œã‹ã‚‰ãªã‚‹è‡ªç„¶è¨€èªå‡¦ç†ã®é‡è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ \n",
            "- GPT-3 175Bã®ã‚ˆã†ãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å†å­¦ç¿’ãŒå›°é›£ã«ãªã‚‹\n",
            "- LoRAã¨ã„ã†æ‰‹æ³•ã‚’ææ¡ˆã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›\n",
            "- LoRAã¯ã€GPT-3 175Bã®å ´åˆã€Adamã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸGPT-3 175Bã¨æ¯”è¼ƒã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’10,000å€ã€GPUãƒ¡ãƒ¢ãƒªã®è¦ä»¶ã‚’3å€ã«å‰Šæ¸›ã§ãã‚‹\n",
            "- LoRAã¯ã€RoBERTaã€DeBERTaã€GPT-2ã€GPT-3ã®ãƒ¢ãƒ‡ãƒ«å“è³ªã«ãŠã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒé«˜ãã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨ã¯ç•°ãªã‚Šè¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹\n",
            "- LoRAã®åŠ¹æœã«ã¤ã„ã¦ã®å®Ÿè¨¼çš„ãªèª¿æŸ»ã‚’æä¾›ã—ã€LoRAã‚’PyTorchãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã™ã‚‹ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã€RoBERTaã€DeBERTaã€GPT-2ã®å®Ÿè£…ã¨ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã™ã‚‹\n",
            "\n",
            "2. INTRODUCTION\n",
            "- è‡ªç„¶è¨€èªå‡¦ç†ã®å¤šãã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€1ã¤ã®å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹ã“ã¨ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹\n",
            "- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†å­¦ç¿’ã™ã‚‹ãŸã‚ã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã‚³ã‚¹ãƒˆãŒé«˜ããªã‚‹\n",
            "- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã§ã‚ã‚Šã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹\n",
            "- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åˆ¶ç´„ã‚’è»½æ¸›ã™ã‚‹\n",
            "- LoRAã¯ã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚„ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯ç•°ãªã‚Šã€è¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„\n",
            "- LoRAã¯ã€ä»–ã®æ‰‹æ³•ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã‚‹\n",
            "\n",
            "3. PROBLEM STATEMENT\n",
            "- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†å­¦ç¿’ã™ã‚‹ãŸã‚ã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã‚³ã‚¹ãƒˆãŒé«˜ããªã‚‹\n",
            "- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã§ã‚ã‚Šã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹\n",
            "- ãƒ©ãƒ³ã‚¯ã®ä½ã„ã‚¢ãƒ€ãƒ—ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¡Œåˆ—ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨è¨ˆç®—ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹\n",
            "- LoRAã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒé«˜ãã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãã€è¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ãƒ¢ãƒ‡ãƒ«ã®å“è³ªãŒåŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™\n",
            "- LoRAã¯ã€PyTorchãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã™ã‚‹ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã€RoBERTaã€DeBERTaã€GPT-2ã®å®Ÿè£…ã¨ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã™ã‚‹\n",
            "\n",
            "4. OUR METHOD\n",
            "- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ—¢å­˜ã®é‡ã¿è¡Œåˆ—ã«æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã™ã‚‹æ‰‹æ³•\n",
            "- LoRAã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç‰¹å®šã®é‡ã¿è¡Œåˆ—ã«ã®ã¿é©ç”¨ã™ã‚‹\n",
            "- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã™ã‚‹ã“ã¨ã§ã€ãƒ¡ãƒ¢ãƒªã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã€ã‚¿ã‚¹ã‚¯ã®åˆ‡ã‚Šæ›¿ãˆã‚’åŠ¹ç‡åŒ–ã™ã‚‹\n",
            "- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åˆ¶ç´„ã‚’è»½æ¸›ã™ã‚‹\n",
            "- LoRAã¯ã€æ¨è«–æ™‚ã«è¿½åŠ ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å°å…¥ã›ãšã€ä»–ã®æ‰‹æ³•ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã‚‹\n",
            "\n",
            "5. EMPIRICAL EXPERIMENTS\n",
            "- RoBERTaã€DeBERTaã€GPT-2ã€GPT-3ã®ãƒ¢ãƒ‡ãƒ«ã«LoRAã‚’é©ç”¨ã—ã€GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚„ä»–ã®ã‚¿ã‚¹ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹\n",
            "- LoRAã¯ã€ä»–ã®æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã™\n",
            "- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹\n",
            "- LoRAã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦è¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„\n",
            "- LoRAã¯ã€PyTorchãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã™ã‚‹ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã€RoBERTaã€DeBERTaã€GPT-2ã®å®Ÿè£…ã¨ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã™ã‚‹\n",
            "\n",
            "6. RELATED WORKS\n",
            "- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹\n",
            "- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®æ”¹å–„æ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹\n",
            "- ä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®åˆ†é‡ã§ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹\n",
            "- LoRAã¯ã€ä»–ã®æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã™\n",
            "- LoRAã¯ã€ä»–ã®æ‰‹æ³•ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã‚‹"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(f'''\n",
        "{response.content}\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "9lCdWxrYUPUX",
        "outputId": "1fef8f9c-48a4-49e6-ac33-1e1b6da1dbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n1. ABSTRACT\n- å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’ã¨ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®é©å¿œã‹ã‚‰ãªã‚‹è‡ªç„¶è¨€èªå‡¦ç†ã®é‡è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ \n- GPT-3 175Bã®ã‚ˆã†ãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å†å­¦ç¿’ãŒå›°é›£ã«ãªã‚‹\n- LoRAã¨ã„ã†æ‰‹æ³•ã‚’ææ¡ˆã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›\n- LoRAã¯ã€GPT-3 175Bã®å ´åˆã€Adamã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸGPT-3 175Bã¨æ¯”è¼ƒã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’10,000å€ã€GPUãƒ¡ãƒ¢ãƒªã®è¦ä»¶ã‚’3å€ã«å‰Šæ¸›ã§ãã‚‹\n- LoRAã¯ã€RoBERTaã€DeBERTaã€GPT-2ã€GPT-3ã®ãƒ¢ãƒ‡ãƒ«å“è³ªã«ãŠã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒé«˜ãã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨ã¯ç•°ãªã‚Šè¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹\n- LoRAã®åŠ¹æœã«ã¤ã„ã¦ã®å®Ÿè¨¼çš„ãªèª¿æŸ»ã‚’æä¾›ã—ã€LoRAã‚’PyTorchãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã™ã‚‹ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã€RoBERTaã€DeBERTaã€GPT-2ã®å®Ÿè£…ã¨ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã™ã‚‹\n\n2. INTRODUCTION\n- è‡ªç„¶è¨€èªå‡¦ç†ã®å¤šãã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€1ã¤ã®å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹ã“ã¨ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹\n- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†å­¦ç¿’ã™ã‚‹ãŸã‚ã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã‚³ã‚¹ãƒˆãŒé«˜ããªã‚‹\n- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã§ã‚ã‚Šã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹\n- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åˆ¶ç´„ã‚’è»½æ¸›ã™ã‚‹\n- LoRAã¯ã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚„ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ã¯ç•°ãªã‚Šã€è¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„\n- LoRAã¯ã€ä»–ã®æ‰‹æ³•ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã‚‹\n\n3. PROBLEM STATEMENT\n- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†å­¦ç¿’ã™ã‚‹ãŸã‚ã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã‚³ã‚¹ãƒˆãŒé«˜ããªã‚‹\n- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã§ã‚ã‚Šã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹\n- ãƒ©ãƒ³ã‚¯ã®ä½ã„ã‚¢ãƒ€ãƒ—ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¡Œåˆ—ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨è¨ˆç®—ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹\n- LoRAã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒé«˜ãã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãã€è¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ãƒ¢ãƒ‡ãƒ«ã®å“è³ªãŒåŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™\n- LoRAã¯ã€PyTorchãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã™ã‚‹ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã€RoBERTaã€DeBERTaã€GPT-2ã®å®Ÿè£…ã¨ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã™ã‚‹\n\n4. OUR METHOD\n- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ—¢å­˜ã®é‡ã¿è¡Œåˆ—ã«æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã™ã‚‹æ‰‹æ³•\n- LoRAã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç‰¹å®šã®é‡ã¿è¡Œåˆ—ã«ã®ã¿é©ç”¨ã™ã‚‹\n- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã™ã‚‹ã“ã¨ã§ã€ãƒ¡ãƒ¢ãƒªã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã€ã‚¿ã‚¹ã‚¯ã®åˆ‡ã‚Šæ›¿ãˆã‚’åŠ¹ç‡åŒ–ã™ã‚‹\n- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åˆ¶ç´„ã‚’è»½æ¸›ã™ã‚‹\n- LoRAã¯ã€æ¨è«–æ™‚ã«è¿½åŠ ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å°å…¥ã›ãšã€ä»–ã®æ‰‹æ³•ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã‚‹\n\n5. EMPIRICAL EXPERIMENTS\n- RoBERTaã€DeBERTaã€GPT-2ã€GPT-3ã®ãƒ¢ãƒ‡ãƒ«ã«LoRAã‚’é©ç”¨ã—ã€GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚„ä»–ã®ã‚¿ã‚¹ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹\n- LoRAã¯ã€ä»–ã®æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã™\n- LoRAã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹\n- LoRAã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦è¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„\n- LoRAã¯ã€PyTorchãƒ¢ãƒ‡ãƒ«ã¨çµ±åˆã™ã‚‹ãŸã‚ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨ã€RoBERTaã€DeBERTaã€GPT-2ã®å®Ÿè£…ã¨ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æä¾›ã™ã‚‹\n\n6. RELATED WORKS\n- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã®åˆ†é‡ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹\n- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®æ”¹å–„æ‰‹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹\n- ä½ãƒ©ãƒ³ã‚¯æ§‹é€ ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®åˆ†é‡ã§ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹\n- LoRAã¯ã€ä»–ã®æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã™\n- LoRAã¯ã€ä»–ã®æ‰‹æ³•ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã‚‹\n"
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_token(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miirURajMnYB",
        "outputId": "bfbf29a8-c2da-4082-f511-8ee2b287f67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total token: 1812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpWCyyjhM1jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IfvNOYPJM1or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ã†å ´åˆ"
      ],
      "metadata": {
        "id": "woCuP9KTM1rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = summarizer(content, use_system_message=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqioJTfFMnbF",
        "outputId": "e7a55636-19cd-4b40-ca87-b8ffd205028a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. ABSTRACT\n",
            "- è«–æ–‡ã®è¦ç‚¹: LoRAã¯ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œã•ã›ã‚‹ãŸã‚ã®ä½ãƒ©ãƒ³ã‚¯é©å¿œæ‰‹æ³•ã§ã™ã€‚LoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å‡çµã—ã€å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚LoRAã¯ã€GPT-3 175Bã¨æ¯”è¼ƒã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’10,000å€ã€GPUãƒ¡ãƒ¢ãƒªã®è¦ä»¶ã‚’3å€ã«å‰Šæ¸›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€LoRAã¯ã€RoBERTaã€DeBERTaã€GPT-2ã€GPT-3ã®ãƒ¢ãƒ‡ãƒ«å“è³ªã«ãŠã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒé«˜ãã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨ã¯ç•°ãªã‚Šè¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€åŒç­‰ä»¥ä¸Šã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚\n",
            "\n",
            "2. INTRODUCTION\n",
            "- è«–æ–‡ã®èƒŒæ™¯: è‡ªç„¶è¨€èªå‡¦ç†ã®é‡è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯ã€ä¸€èˆ¬çš„ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã§ã®å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’ã¨ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®é©å¿œã§ã™ã€‚ã—ã‹ã—ã€ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰å­¦ç¿’ã™ã‚‹ã«ã¤ã‚Œã¦ã€å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†å­¦ç¿’ã™ã‚‹å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯å®Ÿç¾å›°é›£ã«ãªã‚Šã¾ã™ã€‚ãã“ã§ã€LoRAã§ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å‡çµã—ã€å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚\n",
            "\n",
            "3. PROBLEM STATEMENT\n",
            "- å•é¡Œã®æ¦‚è¦: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå…ƒã®ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ã ã‘å¿…è¦ã«ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã«ãŠã‘ã‚‹é‡è¦ãªèª²é¡Œã¨ãªã‚Šã¾ã™ã€‚LoRAã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
            "\n",
            "4. OUR METHOD\n",
            "- ãƒ¡ã‚½ãƒƒãƒ‰ã®æ¦‚è¦: LoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚LoRAã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åˆ¶ç´„ã‚’ç·©å’Œã—ã¾ã™ã€‚ã¾ãŸã€LoRAã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¿½åŠ ã—ãªã„ãŸã‚ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ™‚ã®åŠ¹ç‡ã‚‚å‘ä¸Šã•ã›ã¾ã™ã€‚\n",
            "\n",
            "5. EMPIRICAL EXPERIMENTS\n",
            "- å®Ÿé¨“çµæœã®æ¦‚è¦: LoRAã¯ã€RoBERTaã€DeBERTaã€GPT-2ã€GPT-3ã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã—ãªãŒã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€GPT-3 175Bã«ãŠã„ã¦ã‚‚ã€LoRAã¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚\n",
            "\n",
            "6. RELATED WORKS\n",
            "- é–¢é€£ç ”ç©¶ã®æ¦‚è¦: LoRAã¯ã€ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®é«˜ã„é©å¿œæ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦ã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚ã¾ãŸã€LoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä¸‹æµã®ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹ãŸã‚ã®åŸºæœ¬åŸå‰‡ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã‚‚ã®ã§ã™ã€‚"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(f'''\n",
        "{response.content}\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "ls0ZMPuRMneI",
        "outputId": "e3c84e24-16f4-48f4-ffcf-c97ef2376608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n1. ABSTRACT\n- è«–æ–‡ã®è¦ç‚¹: LoRAã¯ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œã•ã›ã‚‹ãŸã‚ã®ä½ãƒ©ãƒ³ã‚¯é©å¿œæ‰‹æ³•ã§ã™ã€‚LoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å‡çµã—ã€å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚LoRAã¯ã€GPT-3 175Bã¨æ¯”è¼ƒã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’10,000å€ã€GPUãƒ¡ãƒ¢ãƒªã®è¦ä»¶ã‚’3å€ã«å‰Šæ¸›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€LoRAã¯ã€RoBERTaã€DeBERTaã€GPT-2ã€GPT-3ã®ãƒ¢ãƒ‡ãƒ«å“è³ªã«ãŠã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå°‘ãªãã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒé«˜ãã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨ã¯ç•°ãªã‚Šè¿½åŠ ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€åŒç­‰ä»¥ä¸Šã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚\n\n2. INTRODUCTION\n- è«–æ–‡ã®èƒŒæ™¯: è‡ªç„¶è¨€èªå‡¦ç†ã®é‡è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¯ã€ä¸€èˆ¬çš„ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã§ã®å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’ã¨ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸ã®é©å¿œã§ã™ã€‚ã—ã‹ã—ã€ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰å­¦ç¿’ã™ã‚‹ã«ã¤ã‚Œã¦ã€å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å†å­¦ç¿’ã™ã‚‹å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯å®Ÿç¾å›°é›£ã«ãªã‚Šã¾ã™ã€‚ãã“ã§ã€LoRAã§ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å‡çµã—ã€å„å±¤ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã™ã€‚\n\n3. PROBLEM STATEMENT\n- å•é¡Œã®æ¦‚è¦: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå…ƒã®ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ã ã‘å¿…è¦ã«ãªã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã«ãŠã‘ã‚‹é‡è¦ãªèª²é¡Œã¨ãªã‚Šã¾ã™ã€‚LoRAã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n\n4. OUR METHOD\n- ãƒ¡ã‚½ãƒƒãƒ‰ã®æ¦‚è¦: LoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚LoRAã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åˆ¶ç´„ã‚’ç·©å’Œã—ã¾ã™ã€‚ã¾ãŸã€LoRAã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¿½åŠ ã—ãªã„ãŸã‚ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ™‚ã®åŠ¹ç‡ã‚‚å‘ä¸Šã•ã›ã¾ã™ã€‚\n\n5. EMPIRICAL EXPERIMENTS\n- å®Ÿé¨“çµæœã®æ¦‚è¦: LoRAã¯ã€RoBERTaã€DeBERTaã€GPT-2ã€GPT-3ã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã—ãªãŒã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€GPT-3 175Bã«ãŠã„ã¦ã‚‚ã€LoRAã¯ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚\n\n6. RELATED WORKS\n- é–¢é€£ç ”ç©¶ã®æ¦‚è¦: LoRAã¯ã€ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®é«˜ã„é©å¿œæ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦ã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚ã¾ãŸã€LoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä¸‹æµã®ã‚¿ã‚¹ã‚¯ã«é©å¿œã•ã›ã‚‹ãŸã‚ã®åŸºæœ¬åŸå‰‡ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã‚‚ã®ã§ã™ã€‚\n"
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBmi_-wOVQNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template=\"\"\"\n",
        "æ¬¡ã®è«–æ–‡ã‹ã‚‰ä¸‹è¨˜ã®è³ªå•ã«ç­”ãˆã‚ˆ\n",
        "è«–æ–‡:{thesis}\n",
        "\n",
        "è³ªå•:{question}\n",
        "\"\"\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(thesis=content, question=\"ã©ã®ã‚ˆã†ãªè©•ä¾¡ã‚’è¡Œã£ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®å“è³ªãŒç¶­æŒã§ãã‚‹ã“ã¨ã‚’ç¢ºã‹ã‚ãŸã®ã§ã™ã‹ï¼Ÿ\").to_messages()\n",
        "response = llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQEGSoX5VLjT",
        "outputId": "446034d2-893b-451e-d644-a064bbc56fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è«–æ–‡ã§ã¯ã€LoRAã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€RoBERTaã€DeBERTaã€GPT-2ã€ãŠã‚ˆã³GPT-3ã®ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã§ã®å®Ÿé¨“ã‚’è¡Œã„ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã¯ã€GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€E2E NLG Challengeã€WikiSQLã€MultiNLIã€SAMSumãªã©ã®ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€LoRAãŒãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®æ€§èƒ½ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€LoRAãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã“ã¨ã‚‚ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(f'''\n",
        "{response.content}\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "QnX49K5FMngg",
        "outputId": "0b1f2ccb-4024-4a93-f0ab-b8bf49bf2d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nè«–æ–‡ã§ã¯ã€LoRAã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€RoBERTaã€DeBERTaã€GPT-2ã€ãŠã‚ˆã³GPT-3ã®ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã§ã®å®Ÿé¨“ã‚’è¡Œã„ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã¯ã€GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€E2E NLG Challengeã€WikiSQLã€MultiNLIã€SAMSumãªã©ã®ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€LoRAãŒãƒ•ãƒ«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®æ€§èƒ½ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€LoRAãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã“ã¨ã‚‚ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚\n"
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5phbBn-5M55w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkF7F0zhM6ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-X6VInPlM6zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ar_e8HcYM59t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"less_is_more.pdf\")\n",
        "content = preprocess_text(loader)"
      ],
      "metadata": {
        "id": "fKVotZe3M6Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1PZJTb59M8Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ã‚ãªã„å ´åˆ\n",
        "response = summarizer(content, use_system_message=False)"
      ],
      "metadata": {
        "id": "gbkz7BDQM8T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ã†å ´åˆ\n",
        "response = summarizer(content, use_system_message=True)"
      ],
      "metadata": {
        "id": "ZSATJrI3M-OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9ACjYtCNCz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fXeS_t4NC3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWS9fx4ENC6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# è³ªå•ã‚‚ã§ãã‚‹"
      ],
      "metadata": {
        "id": "mV9TJDOENDot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_template=\"\"\"\n",
        "æ¬¡ã®è«–æ–‡ã‹ã‚‰ä¸‹è¨˜ã®è³ªå•ã«ç­”ãˆã‚ˆ\n",
        "è«–æ–‡:{thesis}\n",
        "\n",
        "è³ªå•:{question}\n",
        "\"\"\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "prompt = chat_prompt.format_prompt(thesis=content, question=\"ç´„1,000ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã¯å…·ä½“çš„ã«ã©ã†ã„ã†ã‚‚ã®ã§ã™ã‹ï¼Ÿ\").to_messages()\n",
        "response = llm(prompt)"
      ],
      "metadata": {
        "id": "DmirIZgyNC8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24qUnUxAM-SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sx3rIIQaM-Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWP479aiM-YY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}